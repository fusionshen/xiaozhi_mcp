# main.py
import re
import logging
import os
import pickle
from typing import List, Optional
import pandas as pd
import numpy as np
import jieba
import torch
import time

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from rapidfuzz import process, fuzz

try:
    from sentence_transformers import SentenceTransformer
    HAVE_ST = True
except Exception:
    HAVE_ST = False

# ================= Êó•ÂøóÈÖçÁΩÆ =================
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("formula-api")

# ================= FastAPI ÂàùÂßãÂåñ =================
app = FastAPI(title="Formula Query API - Jieba + Semantic (weighted hybrid)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

# ================= ÂÖ®Â±ÄÈÖçÁΩÆ =================
CSV_PATH = os.environ.get("FORMULA_CSV", "FORMULAINFO_202503121558.csv")
EMBEDDING_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"
EMBEDDING_CACHE_PATH = "formula_embeddings.pkl"
ENV_EMBEDDING_DEVICE = os.environ.get("EMBEDDING_DEVICE", "").lower()

# üîπÂ§öÂÖ≥ÈîÆËØçÂä†ÊùÉË°®ÔºàÊîØÊåÅÂè†Âä†Ôºâ
TEXT_SCORE_WEIGHT_MAP = {
    "ÂÆûÁª©": 0.08,  # ÂÆûÁª©‰ºòÂÖà
    "Êä•Âá∫": 0.01,  # Êä•Âá∫Ê¨°‰ºò
    "ËÆ°Âàí": -0.01,  # ËÆ°ÂàíÁ®çÂº±
    "Á¥ØËÆ°": -0.02,  # Á¥ØËÆ°Áï•ÂáèÂàÜ
}
ENABLE_TEXT_SCORE_WEIGHT = True

# ================= ÂÖ®Â±ÄÂèòÈáè =================
df: Optional[pd.DataFrame] = None
_formulanames_raw: List[str] = []
_formulanames_clean: List[str] = []
_formulanames_tokens: List[str] = []
_embeddings: Optional[np.ndarray] = None
_embedding_model = None

# ===========================================================
# Â∑•ÂÖ∑ÂáΩÊï∞
# ===========================================================
def normalize_text(s: str) -> str:
    """Ê†áÂáÜÂåñÊñáÊú¨ÔºöÂéªÁ¨¶Âè∑„ÄÅÁ©∫Ê†º"""
    if s is None:
        return ""
    s = str(s).strip().strip('"').strip("'")
    s = s.replace("#", " ")
    s = re.sub(r"[^\w\u4e00-\u9fff]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def tokens_by_jieba(s: str) -> str:
    """Áî® jieba ÂàÜËØç"""
    if not s:
        return ""
    segs = jieba.cut(s, cut_all=False)
    return " ".join([t for t in segs if t.strip()])


def l2_normalize_matrix(mat: np.ndarray) -> np.ndarray:
    """L2 ÂΩí‰∏ÄÂåñÁü©Èòµ"""
    norms = np.linalg.norm(mat, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    return mat / norms


def select_embedding_device() -> str:
    """Ê£ÄÊµãÂèØÁî®ËÆæÂ§áÔºö‰ºòÂÖà CUDA GPU"""
    device = "cpu"
    if ENV_EMBEDDING_DEVICE in ["cuda", "mps", "cpu"]:
        device = ENV_EMBEDDING_DEVICE
        logger.info(f"Using embedding device from environment: {device}")
    else:
        if torch.cuda.is_available():
            device = "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = "mps"
        logger.info(f"Auto-selected embedding device: {device}")
    return device


# ===========================================================
# ÂêØÂä®Âä†ËΩΩ CSV ‰∏é Embeddings
# ===========================================================
@app.on_event("startup")
def load_csv_and_prepare():
    """ÂêØÂä®Êó∂Âä†ËΩΩ CSV + ÂêëÈáèÁºìÂ≠ò"""
    global df, _formulanames_raw, _formulanames_clean, _formulanames_tokens
    global HAVE_ST, _embedding_model, _embeddings

    start_time = time.time()
    logger.info("üîÑ Initializing formula data...")

    # ---- Âä†ËΩΩ CSV ----
    try:
        try:
            df = pd.read_csv(CSV_PATH, dtype=str, quoting=3, engine="python", on_bad_lines="skip")
        except Exception:
            df = pd.read_csv(CSV_PATH, sep="\t", dtype=str, quoting=3, engine="python", on_bad_lines="skip")

        df.columns = [c.strip().replace('"', '') for c in df.columns]
        if not {"FORMULAID", "FORMULANAME"}.issubset(df.columns):
            raise RuntimeError(f"CSV Áº∫Â∞ëÂøÖË¶ÅÂàó: {list(df.columns)}")

        df = df[["FORMULAID", "FORMULANAME"]].fillna("")
        _formulanames_raw = df["FORMULANAME"].astype(str).tolist()
        _formulanames_clean = [normalize_text(s) for s in _formulanames_raw]
        _formulanames_tokens = [tokens_by_jieba(s) for s in _formulanames_clean]

        logger.info(f"‚úÖ Loaded {len(df)} formulas. Tokenization ready.")
    except Exception as e:
        logger.exception("‚ùå Failed to load CSV")
        raise RuntimeError(f"Failed to load CSV: {e}")

    # ---- Âä†ËΩΩÊàñÁîüÊàê Embeddings ----
    if HAVE_ST:
        device = select_embedding_device()
        try:
            _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
            if os.path.exists(EMBEDDING_CACHE_PATH):
                with open(EMBEDDING_CACHE_PATH, "rb") as f:
                    cached_data = pickle.load(f)
                if cached_data.get("formula_count") == len(_formulanames_raw):
                    _embeddings = cached_data["embeddings"]
                    logger.info(f"‚úÖ Loaded embeddings from cache ({_embeddings.shape}) in {time.time()-start_time:.2f}s")
                    return
                else:
                    logger.warning("‚ö†Ô∏è Embedding cache formula count mismatch, recalculating embeddings...")

            # ÈáçÊñ∞ËÆ°ÁÆó
            emb_list = _embedding_model.encode(
                _formulanames_raw,
                batch_size=64,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            _embeddings = l2_normalize_matrix(np.asarray(emb_list, dtype=np.float32))
            with open(EMBEDDING_CACHE_PATH, "wb") as f:
                pickle.dump({"formula_count": len(_formulanames_raw), "embeddings": _embeddings}, f)

            logger.info(f"‚úÖ Computed and cached embeddings ({_embeddings.shape}) in {time.time()-start_time:.2f}s")

        except Exception as e:
            logger.exception("‚ùå Failed to load or compute embeddings. Semantic mode disabled.")
            HAVE_ST = False
            _embedding_model = None
            _embeddings = None
    else:
        logger.warning("‚ö†Ô∏è sentence-transformers not installed ‚Äî semantic mode DISABLED.")


# ===========================================================
# Âä†ÊùÉÂáΩÊï∞
# ===========================================================
def apply_text_weights(formula_name: str, base_score: float) -> float:
    """ÂØπÂåÖÂê´ÁâπÂÆöÂÖ≥ÈîÆËØçÁöÑÂêçÁß∞Âä†ÊùÉÔºåÂèØÂè†Âä†"""
    if not ENABLE_TEXT_SCORE_WEIGHT or base_score <= 0:
        return base_score
    weighted_score = base_score
    for key, w in TEXT_SCORE_WEIGHT_MAP.items():
        if key in formula_name:
            weighted_score *= (1 + w)
    return weighted_score


# ===========================================================
# Ê£ÄÁ¥¢ÂáΩÊï∞
# ===========================================================
def fuzzy_search(user_input: str, topn: int = 5):
    """Ê®°Á≥äÊ£ÄÁ¥¢"""
    key_clean = normalize_text(user_input)
    key_tokens = tokens_by_jieba(key_clean)
    if not key_tokens:
        return []

    results = process.extract(key_tokens, _formulanames_tokens, scorer=fuzz.token_set_ratio, limit=topn * 3)
    candidates = []
    for rank, (match_text, score, match_index) in enumerate(results, start=1):
        row = df.iloc[match_index]
        final_score = apply_text_weights(row["FORMULANAME"], float(score))
        candidates.append({
            "number": rank,
            "FORMULAID": row["FORMULAID"],
            "FORMULANAME": row["FORMULANAME"],
            "score": round(final_score, 4),
            "match_kind": "fuzzy_token_set"
        })
    return sorted(candidates, key=lambda x: x["score"], reverse=True)[:topn]


def semantic_search(user_input: str, topn: int = 5):
    """ËØ≠‰πâÊ£ÄÁ¥¢"""
    if not HAVE_ST or _embeddings is None or _embedding_model is None:
        raise RuntimeError("Semantic mode not available.")
    vec = _embedding_model.encode([user_input], convert_to_numpy=True).astype(np.float32)
    vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)
    sims = np.dot(_embeddings, vec[0])
    top_idx = np.argsort(-sims)[:topn * 3]
    candidates = []
    for rank, idx in enumerate(top_idx, start=1):
        row = df.iloc[int(idx)]
        base_score = float(sims[idx]) * 100.0
        final_score = apply_text_weights(row["FORMULANAME"], base_score)
        candidates.append({
            "number": rank,
            "FORMULAID": row["FORMULAID"],
            "FORMULANAME": row["FORMULANAME"],
            "score": round(final_score, 4),
            "match_kind": "semantic_cosine"
        })
    return sorted(candidates, key=lambda x: x["score"], reverse=True)[:topn]


def hybrid_search(user_input: str, topn: int = 5, fuzzy_weight: float = 0.4, semantic_weight: float = 0.6):
    """Ê∑∑ÂêàÊ£ÄÁ¥¢"""
    fuzzy_candidates = fuzzy_search(user_input, topn=topn * 3)
    if not HAVE_ST or _embeddings is None:
        return fuzzy_candidates[:topn]

    vec = _embedding_model.encode([user_input], convert_to_numpy=True).astype(np.float32)
    vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)
    sims = np.dot(_embeddings, vec[0]) * 100.0

    merged = []
    for c in fuzzy_candidates:
        fid = c["FORMULAID"]
        matched_rows = df.index[df["FORMULAID"] == fid].tolist()
        if not matched_rows:
            continue
        idx = int(matched_rows[0])
        semantic_score = float(sims[idx])
        fuzzy_score = float(c["score"])

        # üîπ Âä®ÊÄÅË∞ÉÊï¥ÊùÉÈáçÔºàÂΩìÊ®°Á≥äÂ∫¶È´òÊó∂ÔºåËØ≠‰πâÂç†ÊØîÊèêÂçáÔºâ
        if fuzzy_score > 95:
            fuzzy_weight, semantic_weight = 0.4, 0.6

        final_score = fuzzy_weight * fuzzy_score + semantic_weight * semantic_score
        final_score = apply_text_weights(df.iloc[idx]["FORMULANAME"], final_score)
        merged.append((final_score, fuzzy_score, semantic_score, idx))

    merged.sort(key=lambda x: x[0], reverse=True)
    candidates = []
    for rank, (final_score, fuzzy_score, semantic_score, idx) in enumerate(merged[:topn], start=1):
        row = df.iloc[idx]
        candidates.append({
            "number": rank,
            "FORMULAID": row["FORMULAID"],
            "FORMULANAME": row["FORMULANAME"],
            "score": round(float(final_score), 4),
            "fuzzy_score": round(float(fuzzy_score), 4),
            "semantic_score": round(float(semantic_score), 4),
            "match_kind": "hybrid"
        })
    return candidates


# ===========================================================
# API Êé•Âè£
# ===========================================================
@app.get("/formula_query")
def formula_query(
    user_input: str = Query(..., description="User input: keyword or exact formula name"),
    topn: int = Query(5, ge=1, le=50, description="Number of candidates to return"),
    method: str = Query("hybrid", description="Search method: fuzzy | semantic | hybrid")
):
    """Áªü‰∏ÄÊü•ËØ¢Êé•Âè£"""
    user_input = user_input.strip().strip('"').strip("'")
    if not user_input:
        return JSONResponse(content={"done": False, "message": "Empty input."})

    # ---------- Á≤æÁ°ÆÂåπÈÖç ----------
    exact = df[df["FORMULANAME"] == user_input]
    if exact.empty:
        clean_input = normalize_text(user_input)
        matches_idx = [i for i, v in enumerate(_formulanames_clean) if v == clean_input]
        if matches_idx:
            exact = pd.DataFrame([df.iloc[matches_idx[0]]])
    if not exact.empty:
        exact_matches = exact[["FORMULAID", "FORMULANAME"]].to_dict(orient="records")
        return JSONResponse(content={
            "done": True,
            "message": f"Exact match found: {exact_matches[0]['FORMULANAME']}",
            "exact_matches": exact_matches
        })

    # ---------- Ê®°Á≥ä / ËØ≠‰πâ / Ê∑∑Âêà ----------
    try:
        method = method.lower()
        if method == "fuzzy":
            candidates = fuzzy_search(user_input, topn=topn)
        elif method == "semantic":
            candidates = semantic_search(user_input, topn=topn)
        elif method == "hybrid":
            candidates = hybrid_search(user_input, topn=topn)
        else:
            return JSONResponse(content={"done": False, "message": f"Unknown method: {method}"})
    except Exception as e:
        logger.exception("‚ùå Search error")
        return JSONResponse(content={"done": False, "message": f"Search error: {e}", "candidates": []})

    # ---------- ËæìÂá∫ ----------
    if not candidates:
        return JSONResponse(content={"done": False, "message": "No matches found.", "candidates": []})

    msg_lines = ["Multiple candidates found, choose by number:"]
    for c in candidates:
        if c.get("match_kind") == "hybrid":
            msg_lines.append(
                f"{c['number']}) {c['FORMULANAME']} (final {c['score']}, fuzzy {c['fuzzy_score']}, semantic {c['semantic_score']})"
            )
        else:
            msg_lines.append(f"{c['number']}) {c['FORMULANAME']} (score {c['score']})")

    return JSONResponse(content={
        "done": False,
        "message": "\n".join(msg_lines),
        "candidates": candidates
    })

