import re
import logging
import os
import pickle
from typing import List, Optional
import pandas as pd
import numpy as np
import jieba
import torch
import time

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from rapidfuzz import process, fuzz

from config import (
    EMBEDDING_CACHE_NAME, FORMULA_CSV_NAME, COMBINE_WEIGHT_LIST, DEFAULT_COMBINE_BOOST, ENABLE_TEXT_SCORE_WEIGHT
)

try:
    from sentence_transformers import SentenceTransformer
    HAVE_ST = True
    print(f"âœ… sentence-transformers ç‰ˆæœ¬: 5.1.1")
except Exception as e:
    HAVE_ST = False
    print(f"âŒ sentence-transformers å¯¼å…¥å¤±è´¥: {e}")

# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("formula-api")

# ================= FastAPI åˆå§‹åŒ– =================
app = FastAPI(title="Formula Query API - Jieba + Semantic (weighted hybrid)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

# ================= å…¨å±€è·¯å¾„é…ç½® =================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, ".."))

DATA_DIR = os.path.join(PROJECT_ROOT, "data")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models", "sbert_offline_models")

EMBEDDING_CACHE_PATH = os.path.join(DATA_DIR, EMBEDDING_CACHE_NAME)
FORMULA_CSV_PATH = os.path.join(DATA_DIR, FORMULA_CSV_NAME)

# ---- ç¦»çº¿æ¨¡å‹ä¼˜å…ˆè·¯å¾„ ----
OFFLINE_MODEL_PATH = os.path.join(MODELS_DIR, "86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d")

# ---- åœ¨çº¿æ¨¡å‹å¤‡ç”¨ ----
EMBEDDING_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"

# ç¯å¢ƒå˜é‡è®¾å¤‡æ§åˆ¶
ENV_EMBEDDING_DEVICE = os.environ.get("EMBEDDING_DEVICE", "").lower()


# ================= å…¨å±€å˜é‡ =================
df: Optional[pd.DataFrame] = None
_formulanames_raw: List[str] = []
_formulanames_clean: List[str] = []
_formulanames_tokens: List[str] = []
_embeddings: Optional[np.ndarray] = None
_embedding_model = None
_initialized = False  # âœ… é˜²æ­¢é‡å¤åˆå§‹åŒ–


# ===========================================================
# å·¥å…·å‡½æ•°
# ===========================================================
def normalize_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s).strip().strip('"').strip("'")
    s = s.replace("#", " ")
    s = re.sub(r"[^\w\u4e00-\u9fff]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def tokens_by_jieba(s: str) -> str:
    if not s:
        return ""
    segs = jieba.cut(s, cut_all=False)
    return " ".join([t for t in segs if t.strip()])


def l2_normalize_matrix(mat: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(mat, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    return mat / norms


def select_embedding_device() -> str:
    """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆä¼˜å…ˆç¯å¢ƒå˜é‡ï¼‰"""
    device = "cpu"
    if ENV_EMBEDDING_DEVICE in ["cuda", "mps", "cpu"]:
        device = ENV_EMBEDDING_DEVICE
        logger.info(f"Using embedding device from environment: {device}")
    else:
        if torch.cuda.is_available():
            device = "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = "mps"
        logger.info(f"Auto-selected embedding device: {device}")
    return device


# ===========================
# 1ï¸âƒ£ apply_combine_weights
# ===========================
def apply_combine_weights(formula_name: str, base_score: float, user_input: str = "") -> float:
    """
    æ ¹æ® COMBINE_WEIGHT_LIST åŠ¨æ€æå‡åˆ†æ•°ï¼š
    - å¦‚æœ formula_name åŒ…å«æŸç»„åˆçš„æ‰€æœ‰ terms
    - ä¸”ç”¨æˆ·è¾“å…¥æ²¡æœ‰å®Œå…¨åŒ…å«è¯¥ç»„åˆ
    - æŒ‰ weight æå‡ base_score
    """
    weighted = float(base_score)
    formula_text = str(formula_name or "")
    user_text = str(user_input or "")

    if not ENABLE_TEXT_SCORE_WEIGHT or base_score <= 0:
        return weighted

    # æŒ‰ weight é™åºéå†ç»„åˆï¼Œä¿è¯é«˜æƒé‡ä¼˜å…ˆ
    combos = sorted(COMBINE_WEIGHT_LIST, key=lambda c: c.get("weight", 0), reverse=True)

    for combo in combos:
        terms = combo.get("terms", [])
        weight = float(combo.get("weight", 0.0))
        if not terms:
            continue

        # formula_name æ˜¯å¦åŒ…å«è¯¥ç»„åˆæ‰€æœ‰ term
        if all(term in formula_text for term in terms):
            # ç”¨æˆ·è¾“å…¥æ˜¯å¦å·²åŒ…å«è¯¥ç»„åˆ
            if not all(term in user_text for term in terms):
                # åŠ æƒæå‡
                weighted *= (1.0 + weight)

    return weighted


# ===========================================================
# åˆå§‹åŒ–å‡½æ•°ï¼ˆæ ¸å¿ƒæ”¹åŠ¨ï¼‰
# ===========================================================
def initialize():
    """åˆå§‹åŒ–å…¬å¼æ•°æ®ä¸åµŒå…¥ï¼Œåªæ‰§è¡Œä¸€æ¬¡"""
    global df, _formulanames_raw, _formulanames_clean, _formulanames_tokens
    global _embedding_model, _embeddings, HAVE_ST, _initialized

    # âœ… é¿å…é‡å¤åŠ è½½ï¼ˆä» main.py å¯¼å…¥ä¸ä¼šæ‰§è¡Œç¬¬äºŒæ¬¡ï¼‰
    if _initialized:
        logger.info("âœ… formula_api å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åŠ è½½ã€‚")
        return

    start_time = time.time()
    logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–å…¬å¼æ•°æ®ï¼ˆfull loadï¼‰...")

    # ---- åŠ è½½ CSV ----
    if not os.path.exists(FORMULA_CSV_PATH):
        raise RuntimeError(f"âš ï¸ æ‰¾ä¸åˆ°å…¬å¼æ•°æ®æ–‡ä»¶: {os.path.abspath(FORMULA_CSV_PATH)}")

    try:
        df = pd.read_csv(FORMULA_CSV_PATH, dtype=str, quoting=3, engine="python", on_bad_lines="skip")
        df.columns = [c.strip().replace('"', '') for c in df.columns]
        if not {"FORMULAID", "FORMULANAME"}.issubset(df.columns):
            raise RuntimeError(f"CSV ç¼ºå°‘å¿…è¦åˆ—: {list(df.columns)}")
        df = df[["FORMULAID", "FORMULANAME"]].fillna("")
        _formulanames_raw = df["FORMULANAME"].astype(str).tolist()
        _formulanames_clean = [normalize_text(s) for s in _formulanames_raw]
        _formulanames_tokens = [tokens_by_jieba(s) for s in _formulanames_clean]
        _ = list(jieba.cut("æµ‹è¯•"))  # è§¦å‘ jieba åˆå§‹åŒ–
        logger.info(f"âœ… Loaded {len(df)} formulas. Tokenization ready.")
    except Exception as e:
        logger.exception("âŒ Failed to load CSV")
        raise RuntimeError(f"Failed to load CSV: {e}")
    
    print(f"HAVE_ST : {HAVE_ST}")
    # ---- å°è¯•åŠ è½½åµŒå…¥æ¨¡å‹ ----
    if HAVE_ST:
        device = select_embedding_device()
        try:
            # âœ… ä¼˜å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹
            print(f"OFFLINE_MODEL_PATH:{OFFLINE_MODEL_PATH}") 
            if os.path.exists(OFFLINE_MODEL_PATH):
                logger.info(f"ğŸ§© å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {OFFLINE_MODEL_PATH}")
                _embedding_model = SentenceTransformer(OFFLINE_MODEL_PATH, device=device)
                logger.info("âœ… å·²æˆåŠŸåŠ è½½ç¦»çº¿æ¨¡å‹ã€‚")
            else:
                logger.warning("âš ï¸ ç¦»çº¿æ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤åœ¨çº¿æ¨¡å‹ã€‚")
                _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
                logger.info("âœ… å·²åŠ è½½åœ¨çº¿æ¨¡å‹ã€‚")
        except Exception as e:
            logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åœ¨çº¿æ¨¡å‹ã€‚é”™è¯¯: {e}")
            _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
            logger.info("âœ… å·²åŠ è½½åœ¨çº¿æ¨¡å‹ã€‚")

        # ---- åŠ è½½æˆ–ç”ŸæˆåµŒå…¥ç¼“å­˜ ----
        if os.path.exists(EMBEDDING_CACHE_PATH):
            with open(EMBEDDING_CACHE_PATH, "rb") as f:
                cached_data = pickle.load(f)
            if cached_data.get("formula_count") == len(_formulanames_raw):
                _embeddings = cached_data["embeddings"]
                logger.info(f"âœ… Loaded embeddings from cache ({_embeddings.shape})")
            else:
                logger.warning("âš ï¸ Embedding cache formula count mismatch, recalculating...")
                _embeddings = _compute_and_cache_embeddings()
        else:
            _embeddings = _compute_and_cache_embeddings()
    else:
        logger.warning("âš ï¸ sentence-transformers not installed â€” semantic mode DISABLED.")
        _embedding_model = None
        _embeddings = None

    _initialized = True
    logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆï¼Œç”¨æ—¶ {time.time() - start_time:.2f}s")


def _compute_and_cache_embeddings():
    """é‡æ–°è®¡ç®—å¹¶ç¼“å­˜åµŒå…¥"""
    logger.info("ğŸ”„ Computing new embeddings...")
    emb_list = _embedding_model.encode(
        _formulanames_raw, batch_size=64, show_progress_bar=True, convert_to_numpy=True
    )
    embeddings = l2_normalize_matrix(np.asarray(emb_list, dtype=np.float32))
    with open(EMBEDDING_CACHE_PATH, "wb") as f:
        pickle.dump({"formula_count": len(_formulanames_raw), "embeddings": embeddings}, f)
    logger.info(f"âœ… Cached new embeddings ({embeddings.shape})")
    return embeddings


# ===========================
# 2ï¸âƒ£ fuzzy_search
# ===========================
def fuzzy_search(user_input: str, topn: int = 5):
    key_clean = normalize_text(user_input)
    key_tokens = tokens_by_jieba(key_clean)
    if not key_tokens:
        return []

    results = process.extract(key_tokens, _formulanames_tokens, scorer=fuzz.token_set_ratio, limit=topn*3)
    candidates = []
    for rank, (match_text, score, match_index) in enumerate(results, start=1):
        row = df.iloc[match_index]
        clean_name = str(row["FORMULANAME"]).strip().strip('"').strip("'")
        base_score = float(score) / 100.0  # å½’ä¸€åŒ–
        final_score = apply_combine_weights(clean_name, base_score, user_input)
        candidates.append({
            "number": rank,
            "FORMULAID": row["FORMULAID"],
            "FORMULANAME": clean_name,
            "score": round(final_score, 4),
            "match_kind": "fuzzy_token_set"
        })
    return sorted(candidates, key=lambda x: x["score"], reverse=True)[:topn]


# ===========================
# 3ï¸âƒ£ semantic_search
# ===========================
def semantic_search(user_input: str, topn: int = 5):
    if _embedding_model is None or _embeddings is None:
        return []

    vec = _embedding_model.encode([user_input], convert_to_numpy=True).astype(np.float32)
    vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)
    sims = np.dot(_embeddings, vec[0])  # cosine similarity [-1,1]

    candidates = []
    for idx in np.argsort(-sims)[:topn*3]:
        row = df.iloc[idx]
        clean_name = str(row["FORMULANAME"]).strip().strip('"').strip("'")
        base_score = (float(sims[idx]) + 1.0) / 2.0  # [-1,1] -> [0,1]
        final_score = apply_combine_weights(clean_name, base_score, user_input)
        candidates.append({
            "number": len(candidates)+1,
            "FORMULAID": row["FORMULAID"],
            "FORMULANAME": clean_name,
            "score": round(final_score, 4),
            "match_kind": "semantic_cosine"
        })
    return sorted(candidates, key=lambda x: x["score"], reverse=True)[:topn]



# ===========================
# 4ï¸âƒ£ hybrid_search
# ===========================
def hybrid_search(user_input: str, topn: int = 5, fuzzy_weight: float = 0.4, semantic_weight: float = 0.6):
    fuzzy_candidates = fuzzy_search(user_input, topn=topn*3)
    if not HAVE_ST or _embeddings is None:
        return fuzzy_candidates[:topn]

    vec = _embedding_model.encode([user_input], convert_to_numpy=True).astype(np.float32)
    vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)
    sims = np.dot(_embeddings, vec[0])  # [-1,1]

    merged = []
    for c in fuzzy_candidates:
        idx = int(df.index[df["FORMULAID"] == c["FORMULAID"]][0])
        semantic_score = (float(sims[idx]) + 1.0) / 2.0  # [-1,1] -> [0,1]
        fuzzy_score = float(c["score"])  # å·²å½’ä¸€åŒ–
        combined_score = fuzzy_weight * fuzzy_score + semantic_weight * semantic_score
        clean_name = str(df.iloc[idx]["FORMULANAME"]).strip().strip('"').strip("'")
        final_score = apply_combine_weights(clean_name, combined_score, user_input)
        merged.append((final_score, fuzzy_score, semantic_score, idx))

    merged.sort(key=lambda x: x[0], reverse=True)

    candidates = []
    for rank, (final_score, fuzzy_score, semantic_score, idx) in enumerate(merged[:topn], start=1):
        row = df.iloc[idx]
        candidates.append({
            "number": rank,
            "FORMULAID": row["FORMULAID"],
            "FORMULANAME": str(row["FORMULANAME"]).strip().strip('"').strip("'"),
            "score": round(float(final_score), 4),
            "fuzzy_score": round(float(fuzzy_score), 4),
            "semantic_score": round(float(semantic_score), 4),
            "match_kind": "hybrid"
        })

    return candidates

def hierarchical_exact_match(user_input: str, df, combine_weight_list):
    user_input = user_input.strip()

    # æŒ‰ weight é™åºï¼Œä¿è¯ weight é«˜çš„ä¼˜å…ˆ
    combos = sorted(combine_weight_list, key=lambda c: c["weight"], reverse=True)

    for item in combos:
        terms = item["terms"]  # å¯åŠ¨æ€å¤šçº§ï¼Œä¾‹å¦‚ ["å®ç»©","æŠ¥å‡ºå€¼","åœ°åŒºA"]

        # æ‰¾ç”¨æˆ·è¾“å…¥å‘½ä¸­ terms çš„æœ€é•¿å‰ç¼€é•¿åº¦
        prefix_len = 0
        for i, term in enumerate(terms):
            if term in user_input:
                prefix_len += 1
            else:
                break

        # å‰©ä½™å±‚çº§éœ€è¦æ‹¼æ¥
        suffix = "".join(terms[prefix_len:])
        candidate = user_input + suffix if suffix else user_input

        # é¿å…é‡å¤æ‹¼æ¥ï¼Œç›´æ¥æŸ¥æ‰¾ç²¾ç¡®åŒ¹é…
        exact = df[df["FORMULANAME"] == candidate]
        if not exact.empty:
            row = exact.iloc[0]
            return {
                "FORMULAID": row["FORMULAID"],
                "FORMULANAME": row["FORMULANAME"],
            }

    return None


# ===========================================================
# API æ¥å£
# ===========================================================
@app.get("/formula_query")
def formula_query(
    user_input: str = Query(..., description="User input: keyword or exact formula name"),
    topn: int = Query(5, ge=1, le=50, description="Number of candidates to return"),
    method: str = Query("hybrid", description="Search method: fuzzy | semantic | hybrid")
):
    return JSONResponse(content=formula_query_dict(user_input, topn, method))


# ===========================================================
# formula_query_dict æ”¹å†™ç‰ˆ
# ===========================================================
def formula_query_dict(user_input: str, topn: int = 5, method: str = "hybrid") -> dict:
    """
    è¿”å›å€™é€‰å…¬å¼çš„ dictï¼Œè§„åˆ™ï¼š
    1ï¸âƒ£ ç²¾ç¡®åŒ¹é…ä¼˜å…ˆï¼ˆFORMULANAME å®Œå…¨ç­‰äºè¾“å…¥æˆ– normalize_text åç›¸ç­‰ï¼‰
    2ï¸âƒ£ è‹¥æ— ç²¾ç¡®åŒ¹é…ï¼Œæ ¹æ® method è°ƒç”¨ fuzzy / semantic / hybrid
    3ï¸âƒ£ åˆ†æ•°å½’ä¸€åŒ– [0,1]ï¼Œåº”ç”¨ç»„åˆæƒé‡
    4ï¸âƒ£ è¿”å› topn ç»“æœ
    """
    user_input = str(user_input or "").strip().strip('"').strip("'")
    if not user_input:
        return {"done": False, "message": "Empty input.", "candidates": []}

    # 0ï¸âƒ£ å±‚çº§ç²¾ç¡®æŸ¥æ‰¾
    hier = hierarchical_exact_match(user_input, df, COMBINE_WEIGHT_LIST)
    if hier:
        logger.info(f"âœ… Hierarchical exact match: {hier['FORMULANAME']}")
        return {
            "done": True,
            "message": f"Hierarchical exact match: {hier['FORMULANAME']}",
            "exact_matches": [hier]
        }

    # ===== 1ï¸âƒ£ ç²¾ç¡®åŒ¹é… =====
    exact = df[df["FORMULANAME"] == user_input]
    if exact.empty:
        # å°è¯• normalize_text ååŒ¹é…
        clean_input = normalize_text(user_input)
        matches_idx = [i for i, v in enumerate(_formulanames_clean) if v == clean_input]
        if matches_idx:
            exact = pd.DataFrame([df.iloc[matches_idx[0]]])

    if not exact.empty:
        exact_matches = exact[["FORMULAID", "FORMULANAME"]].to_dict(orient="records")
        for item in exact_matches:
            item["FORMULANAME"] = str(item["FORMULANAME"]).strip()
        return {
            "done": True,
            "message": f"Exact match found: {exact_matches[0]['FORMULANAME']}",
            "exact_matches": exact_matches,
            "candidates": exact_matches
        }

    # ===== 2ï¸âƒ£ æ¨¡ç³Š / è¯­ä¹‰ / æ··åˆæœç´¢ =====
    method_str = str(method).lower()
    candidates = []
    try:
        if method_str == "fuzzy":
            candidates = fuzzy_search(user_input, topn=topn)
        elif method_str == "semantic":
            candidates = semantic_search(user_input, topn=topn)
        elif method_str == "hybrid":
            candidates = hybrid_search(user_input, topn=topn)
        else:
            return {"done": False, "message": f"Unknown method: {method_str}", "candidates": []}
    except Exception as e:
        logger.exception("âŒ Search error")
        return {"done": False, "message": f"Search error: {e}", "candidates": []}

    if not candidates:
        return {"done": False, "message": "No matches found.", "candidates": []}

    # ===== 3ï¸âƒ£ æ’åº + åˆ†æ•°å½’ä¸€åŒ– =====
    # åˆ†æ•°å·²ç»åœ¨ fuzzy / semantic / hybrid ä¸­å½’ä¸€åŒ– + åº”ç”¨ç»„åˆæƒé‡
    candidates_sorted = sorted(candidates, key=lambda x: x["score"], reverse=True)[:topn]

    return {
        "done": False,
        "message": f"{len(candidates_sorted)} candidates returned.",
        "candidates": candidates_sorted
    }
    


@app.on_event("startup")
def load_csv_and_prepare():
    """FastAPI å¯åŠ¨æ—¶è‡ªåŠ¨è°ƒç”¨"""
    initialize()

# ===========================================================
# ç‹¬ç«‹è¿è¡Œæ”¯æŒï¼ˆpython formula_api.pyï¼‰
# ===========================================================
if __name__ == "__main__":
    initialize()
    print("âœ… formula_api ç‹¬ç«‹è¿è¡Œæ¨¡å¼å¯åŠ¨å®Œæˆã€‚")
