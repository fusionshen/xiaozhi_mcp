# core/pipeline_context.py
"""
ç»Ÿä¸€ç®¡ç†å„ç”¨æˆ·çš„ ContextGraph å¯¹è±¡ï¼Œå†…å­˜ä¼˜å…ˆ + å¼‚æ­¥æŒä¹…åŒ–
æ”¯æŒï¼š
1. pickle å‹ç¼©æ–‡ä»¶ç”¨äºå¿«é€Ÿæ¢å¤
2. JSON æ–‡ä»¶ç”¨äºè°ƒè¯•å’Œå¯è¯»
"""

import os
import asyncio
import pickle
import gzip
import json
from typing import Dict
from core.context_graph import ContextGraph
import logging
import config

logger = logging.getLogger("pipeline_context")
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# å†…å­˜ç¼“å­˜ï¼šç”¨æˆ· ID -> ContextGraph
_graph_store: Dict[str, ContextGraph] = {}

# å›¾è°±å­˜å‚¨ç›®å½•
GRAPH_DIR = os.path.join(os.path.dirname(__file__), "../data/graphs")
os.makedirs(GRAPH_DIR, exist_ok=True)

# ----------------------
# å·¥å…·å‡½æ•°
# ----------------------
def _get_graph_paths(user_id: str):
    safe_user = user_id.replace("/", "_")
    pkl_path = os.path.join(GRAPH_DIR, f"{safe_user}.pkl.gz")
    json_path = os.path.join(GRAPH_DIR, f"{safe_user}.json")
    return pkl_path, json_path

async def save_graph_async(user_id: str, graph: ContextGraph):
    pkl_path, json_path = _get_graph_paths(user_id)
    try:
        loop = asyncio.get_event_loop()
        # ä¿å­˜ pickle å‹ç¼©æ–‡ä»¶
        await loop.run_in_executor(
            None, lambda: gzip.open(pkl_path, "wb").write(pickle.dumps(graph))
        )
        logger.info(f"ğŸ’¾ å¼‚æ­¥ä¿å­˜ç”¨æˆ·å›¾è°± {user_id} -> {pkl_path}")
    except Exception as e:
        logger.exception(f"âš ï¸ ä¿å­˜ç”¨æˆ· {user_id} å›¾è°±å¤±è´¥: {e}")

    if config.ENABLE_GRAGH_DEBUG_JSON:
        try:
            state = graph.to_state()  # ç¡®ä¿ ContextGraph æœ‰ to_state() æ–¹æ³•
            await loop.run_in_executor(
                None,
                lambda: json.dump(state, open(json_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
            )
            logger.info(f"ğŸ“ ä¿å­˜ JSON è°ƒè¯•æ–‡ä»¶ {user_id} -> {json_path}")
        except Exception as e:
            logger.exception(f"âš ï¸ ä¿å­˜ç”¨æˆ· {user_id} JSON å›¾è°±å¤±è´¥: {e}")

def load_graph_from_file(user_id: str) -> ContextGraph | None:
    pkl_path, _ = _get_graph_paths(user_id)
    if not os.path.exists(pkl_path):
        return None
    try:
        with gzip.open(pkl_path, "rb") as f:
            graph = pickle.load(f)
        logger.info(f"ğŸ“‚ åŠ è½½ç”¨æˆ·å›¾è°± {user_id} <- {pkl_path}")
        return graph
    except Exception as e:
        logger.exception(f"âš ï¸ åŠ è½½ç”¨æˆ· {user_id} å›¾è°±å¤±è´¥: {e}")
        return None

# ----------------------
# æ¥å£
# ----------------------
def get_graph(user_id: str) -> ContextGraph:
    """è·å–ç”¨æˆ·å›¾è°±ï¼Œå¦‚æœå†…å­˜æ²¡æœ‰å°±å°è¯•ä»æ–‡ä»¶åŠ è½½"""
    graph = _graph_store.get(user_id)
    if not graph:
        graph = load_graph_from_file(user_id)
        if not graph:
            graph = ContextGraph()
        _graph_store[user_id] = graph
    return graph

def set_graph(user_id: str, graph: ContextGraph) -> None:
    """æ›´æ–°å†…å­˜ï¼Œå¹¶å¼‚æ­¥ä¿å­˜åˆ°ç£ç›˜ï¼ˆpickle + JSONï¼‰"""
    _graph_store[user_id] = graph
    asyncio.create_task(save_graph_async(user_id, graph))

def remove_graph(user_id: str) -> None:
    """åˆ é™¤ç”¨æˆ·å›¾è°±ï¼ˆå†…å­˜ + æ–‡ä»¶ï¼‰"""
    _graph_store.pop(user_id, None)
    pkl_path, json_path = _get_graph_paths(user_id)
    for path in [pkl_path, json_path]:
        if os.path.exists(path):
            try:
                os.remove(path)
                logger.info(f"ğŸ—‘ åˆ é™¤ç”¨æˆ·å›¾è°±æ–‡ä»¶ {user_id} -> {path}")
            except Exception as e:
                logger.exception(f"âš ï¸ åˆ é™¤ç”¨æˆ·å›¾è°±æ–‡ä»¶å¤±è´¥ {user_id}: {e}")

def all_graphs() -> Dict[str, ContextGraph]:
    """è·å–æ‰€æœ‰ç”¨æˆ·å†…å­˜å›¾è°±"""
    return _graph_store

# ----------------------
# å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰å·²æœ‰å›¾è°±
# ----------------------
async def load_all_graphs():
    files = [f for f in os.listdir(GRAPH_DIR) if f.endswith(".pkl.gz")]
    for f in files:
        user_id = f[:-7]  # å»æ‰ ".pkl.gz"
        g = load_graph_from_file(user_id)
        if g:
            _graph_store[user_id] = g
    logger.info(f"âœ… å·²åŠ è½½ {len(_graph_store)} ä¸ªç”¨æˆ·å›¾è°±")

# ----------------------
# å¯é€‰å®šæ—¶æŒä¹…åŒ–ä»»åŠ¡
# ----------------------
async def persist_all_graphs_task(interval_sec: int = 300):
    while True:
        logger.info(f"â³ å¼€å§‹æ‰¹é‡æŒä¹…åŒ–æ‰€æœ‰ç”¨æˆ·å›¾è°±...")
        for user_id, graph in _graph_store.items():
            await save_graph_async(user_id, graph)
        await asyncio.sleep(interval_sec)
