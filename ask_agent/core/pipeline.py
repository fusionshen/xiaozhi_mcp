# core/pipeline.py

import asyncio
import logging
from core.context_graph import ContextGraph
from core.llm_energy_indicator_parser import parse_user_input
from agent_state import get_state, update_state, default_slots
from core.pipeline_handlers import (
    handle_new_query, handle_compare, handle_expand,
    handle_same_indicator_new_time, handle_list_query
)
from core.pipeline_context import get_graph, set_graph  # âœ… æ–°å¢

logger = logging.getLogger("pipeline")
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    )

async def process_message(user_id: str, message: str, graph_state_dict: dict):
    """èƒ½æºè¯­ä¹‰æŸ¥è¯¢ä¸»å…¥å£ï¼šæ ¹æ® intent åˆ†æµå¤„ç†"""
    user_input = (message or "").strip()
    logger.info(f"ğŸŸ¢ [process_message] user={user_id!r} input={user_input!r}")

    # 1ï¸âƒ£ åŠ è½½ graph å’Œ slots
    graph = get_graph(user_id) or ContextGraph.from_state(graph_state_dict)
    set_graph(user_id, graph)  # âœ… ç¡®ä¿ç¼“å­˜åŒæ­¥
    
    session_state = await get_state(user_id)
    session_state.setdefault("slots", default_slots())
    slots = session_state["slots"]
    intent = slots.get("intent", "new_query")
    logger.info(f"ğŸš¦ æ£€æµ‹åˆ°æ„å›¾: {intent}")

    # ---------- æ ¹æ® intent è°ƒç”¨åˆ†æ”¯ ----------
    if intent == "compare":
        return await handle_compare(user_id, message, graph)
    elif intent == "expand":
        return await handle_expand(user_id, message, graph)
    elif intent == "same_indicator_new_time":
        return await handle_same_indicator_new_time(user_id, message, graph)
    elif intent == "list_query":
        return await handle_list_query(user_id, message, graph)
    else:
        return await handle_new_query(user_id, message, graph)
