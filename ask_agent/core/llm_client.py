import os
for key in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]:
    os.environ.pop(key, None)
import json
import httpx
from langchain.schema import HumanMessage
from config import (
    REMOTE_OLLAMA_URL, REMOTE_MODEL, LOCAL_MODEL
)

# ===================== ChatOllama å…¼å®¹å¯¼å…¥ =====================
try:
    from langchain_ollama import ChatOllama
    print("âœ… Using ChatOllama from langchain-ollama")
except ImportError:
    try:
        from langchain_community.chat_models import ChatOllama
        print("âœ… Using ChatOllama from langchain_community")
    except ImportError:
        from langchain.chat_models import ChatOllama
        print("âš ï¸ Using ChatOllama from old langchain (may be deprecated)")


async def is_remote_ollama_available(base_url: str, timeout: float = 3.0) -> bool:
    """
    æ£€æŸ¥è¿œç¨‹ Ollama æœåŠ¡æ˜¯å¦å¯è®¿é—®ã€‚
    """
    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            resp = await client.get(f"{base_url}/api/tags")
            if resp.status_code == 200:
                #print(f"ğŸŒ Remote Ollama available at {base_url}")
                return True
    except Exception as e:
        print(f"âš ï¸ Remote Ollama not reachable: {e}")
    return False


async def get_llm() -> ChatOllama:
    """
    ä¼˜å…ˆä½¿ç”¨è¿œç¨‹ gemma3:27bï¼Œå¦‚æœè¿œç¨‹ä¸å¯ç”¨åˆ™å›é€€åˆ°æœ¬åœ° qwen2.5:1.5bã€‚
    """
    if await is_remote_ollama_available(REMOTE_OLLAMA_URL):
        #print(f"âœ… Using remote model: {REMOTE_MODEL}")
        return ChatOllama(model=REMOTE_MODEL, base_url=REMOTE_OLLAMA_URL)
    else:
        print(f"ğŸ”„ Falling back to local model: {LOCAL_MODEL}")
        return ChatOllama(model=LOCAL_MODEL)


# ===================== é€šç”¨ LLM è°ƒç”¨å‡½æ•° =====================
async def safe_llm_parse(prompt: str) -> dict:
    """
    è°ƒç”¨ LLM å¹¶å®‰å…¨è§£æ JSON è¾“å‡ºã€‚
    - è‡ªåŠ¨å»æ‰ ``` æˆ– ```json åŒ…è£¹
    - JSON è§£æå¤±è´¥æ—¶ç”¨æ­£åˆ™å…œåº•
    è¿”å›å­—å…¸ï¼š{"indicator": ..., "timeString": ..., "timeType": ...} æˆ–è‡ªå®šä¹‰å­—æ®µ
    """
    llm = await get_llm()
    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        content = resp.generations[0][0].message.content.strip()

        # å»æ‰ ``` åŒ…è£¹
        if content.startswith("```"):
            content = "\n".join(content.splitlines()[1:-1]).strip()

        try:
            result = json.loads(content)
        except json.JSONDecodeError:
            # æ­£åˆ™å…œåº•
            result = {}
            pairs = re.findall(r'"(\w+)"\s*:\s*"([^"]*)"', content)
            for k, v in pairs:
                result[k] = v

        return result

    except Exception as e:
        print("âŒ LLM è°ƒç”¨å¤±è´¥:", e)
        return {}


# ===================== é€šç”¨èŠå¤©å‡½æ•° =====================
async def safe_llm_chat(prompt: str) -> str:
    """
    è®©æ¨¡å‹è‡ªç”±å›ç­”ï¼Œè¿”å›çº¯æ–‡æœ¬ã€‚
    """
    llm = await get_llm()
    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        return resp.generations[0][0].message.content.strip()
    except Exception as e:
        print("âŒ LLM èŠå¤©å¤±è´¥:", e)
        return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"