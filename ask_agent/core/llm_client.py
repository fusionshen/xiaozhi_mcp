# core/llm_client.py
import os
import logging
import re
for key in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]:
    os.environ.pop(key, None)
import json
import httpx
from langchain.schema import HumanMessage
from config import (
    REMOTE_OLLAMA_URL, REMOTE_MODEL, LOCAL_MODEL
)

# æ—¥å¿—é…ç½®ï¼ˆè¢«å¯¼å…¥æ—¶ç¡®ä¿ä»…é…ç½®ä¸€æ¬¡ï¼‰
logger = logging.getLogger("llm_client")
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    )

# ===================== ChatOllama å…¼å®¹å¯¼å…¥ =====================
try:
    from langchain_ollama import ChatOllama
    logger.info("âœ… Using ChatOllama from langchain-ollama")
except ImportError:
    try:
        from langchain_community.chat_models import ChatOllama
        logger.info("âœ… Using ChatOllama from langchain_community")
    except ImportError:
        from langchain.chat_models import ChatOllama
        logger.info("âš ï¸ Using ChatOllama from old langchain (may be deprecated)")


async def is_remote_ollama_available(base_url: str, timeout: float = 3.0) -> bool:
    """
    æ£€æŸ¥è¿œç¨‹ Ollama æœåŠ¡æ˜¯å¦å¯è®¿é—®ã€‚
    """
    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            resp = await client.get(f"{base_url}/api/tags")
            if resp.status_code == 200:
                #print(f"ğŸŒ Remote Ollama available at {base_url}")
                return True
    except Exception as e:
        logger.info(f"âš ï¸ Remote Ollama not reachable: {e}")
    return False


async def get_llm() -> ChatOllama:
    """
    ä¼˜å…ˆä½¿ç”¨è¿œç¨‹ gemma3:27bï¼Œå¦‚æœè¿œç¨‹ä¸å¯ç”¨åˆ™å›é€€åˆ°æœ¬åœ° qwen2.5:1.5bã€‚
    """
    if await is_remote_ollama_available(REMOTE_OLLAMA_URL):
        #print(f"âœ… Using remote model: {REMOTE_MODEL}")
        return ChatOllama(model=REMOTE_MODEL, base_url=REMOTE_OLLAMA_URL)
    else:
        logger.info(f"ğŸ”„ Falling back to local model: {LOCAL_MODEL}")
        return ChatOllama(model=LOCAL_MODEL)


# ===================== é€šç”¨ LLM è°ƒç”¨å‡½æ•° =====================
async def safe_llm_parse(prompt: str) -> dict:
    """
    å®‰å…¨è§£æ LLM è¿”å›å†…å®¹ä¸º JSONã€‚
    æ”¯æŒä»¥ä¸‹åœºæ™¯ï¼š
    - æ¨¡å‹è¿”å›çº¯ JSON
    - æ¨¡å‹è¿”å›å‰åå¸¦è§£é‡Šæ–‡å­—
    - æ¨¡å‹è¾“å‡º markdown ä»£ç å—ï¼ˆå¦‚ ```json ... ```ï¼‰
    """
    llm = await get_llm()
    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        response_text = resp.generations[0][0].message.content.strip()
        print(response_text)

        # ğŸ§¹ æ¸…ç†å¸¸è§åŒ…è£¹å­—ç¬¦
        text = response_text.strip()
        text = text.replace("```json", "").replace("```", "").strip()
        text = text.replace("JSON:", "").replace("json:", "").strip()

        # æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}' â€”â€” ä¿è¯å–åˆ°æœ€å¤–å±‚ JSONï¼ˆæ¯”éè´ªå©ªæ­£åˆ™æ›´ç¨³ï¼‰
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1 and end > start:
            json_str = text[start:end+1]
            try:
                data = json.loads(json_str)
                logger.info("âœ… ä» LLM è¾“å‡ºä¸­æˆåŠŸè§£æ JSONã€‚")
                return data
            except json.JSONDecodeError as e_inner:
                logger.warning("âš ï¸ ä»é¦–å°¾å¤§æ‹¬å·æå–çš„ JSON è§£æå¤±è´¥: %s. å°è¯•æ­£åˆ™å…œåº•ã€‚", e_inner)

        # å…œåº•ï¼šå¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™æ‰¾æ‰€æœ‰ {...} å¹¶ä¾æ¬¡å°è¯•è§£æï¼ˆå¤„ç†å¤š JSON æˆ–åµŒå¥—å¤æ‚è¾“å‡ºï¼‰
        matches = re.findall(r"\{[\s\S]*?\}", text)
        for m in matches:
            try:
                data = json.loads(m)
                logger.info("âœ… æ­£åˆ™å…œåº•è§£æåˆ° JSONã€‚")
                return data
            except json.JSONDecodeError:
                continue

        # å†å…œåº•ï¼škey:value ç®€å•è§£æï¼ˆä¿å®ˆï¼‰
        pairs = re.findall(r'"(\w+)"\s*:\s*"([^"]*)"', text)
        if pairs:
            data = {k: v for k, v in pairs}
            logger.warning("âš ï¸ ä½¿ç”¨æ­£åˆ™é”®å€¼å¯¹å…œåº•è§£æ JSONã€‚")
            return data

        logger.warning("âš ï¸ æœªè¯†åˆ«åˆ° JSON æ ¼å¼ï¼Œè¿”å›ç©º dictã€‚åŸæ–‡: %s", text[:400])
        return {}
    except Exception as e:
        logger.exception("âŒ safe_llm_parse è§£æå¤±è´¥: %s", e)
        return {}


# ===================== é€šç”¨èŠå¤©å‡½æ•° =====================
async def safe_llm_chat(prompt: str) -> str:
    """
    è®©æ¨¡å‹è‡ªç”±å›ç­”ï¼Œè¿”å›çº¯æ–‡æœ¬ã€‚
    """
    llm = await get_llm()
    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        return resp.generations[0][0].message.content.strip()
    except Exception as e:
        logger.exception("âŒ LLM èŠå¤©å¤±è´¥:", e)
        return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"