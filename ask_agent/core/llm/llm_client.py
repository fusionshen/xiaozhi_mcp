# core/llm_client.py
import os
import logging
import re
import json
import httpx
from langchain.schema import HumanMessage
from config import REMOTE_OLLAMA_URL, REMOTE_MODEL, LOCAL_MODEL

# ===================== å¼ºåˆ¶ç¦ç”¨ç³»ç»Ÿä»£ç† =====================
for key in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]:
    os.environ.pop(key, None)

# ===================== Logger =====================
logger = logging.getLogger("llm_client")
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    )

# ===================== ChatOllama å…¼å®¹å¯¼å…¥ =====================
try:
    from langchain_ollama import ChatOllama
    logger.info("âœ… Using ChatOllama from langchain-ollama")
except ImportError:
    try:
        from langchain_community.chat_models import ChatOllama
        logger.info("âœ… Using ChatOllama from langchain_community")
    except ImportError:
        from langchain.chat_models import ChatOllama
        logger.info("âš ï¸ Using ChatOllama from old langchain (may be deprecated)")

# ===================== å…¨å±€å…±äº«ç›´è¿ AsyncClient =====================
_global_client: httpx.AsyncClient | None = None

def get_global_client(timeout: float = 10.0) -> httpx.AsyncClient:
    """
    è¿”å›å…¨å±€å…±äº« AsyncClientï¼Œä¿è¯å®Œå…¨ç›´è¿è¿œç¨‹ Ollamaï¼Œä¸èµ°ç³»ç»Ÿä»£ç†ã€‚
    """
    global _global_client
    if _global_client is None:
        transport = httpx.AsyncHTTPTransport(retries=0)
        _global_client = httpx.AsyncClient(
            timeout=timeout,
            transport=transport,
            trust_env=False,  # â­ ä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†
        )
    return _global_client

# ===================== è‡ªå®šä¹‰ ChatOllama =====================
class DirectChatOllama(ChatOllama):
    """
    å¼ºåˆ¶ç›´è¿è¿œç¨‹ Ollamaï¼Œå®Œå…¨å¿½ç•¥ç³»ç»Ÿä»£ç†ã€‚
    """
    def __init__(self, *args, **kwargs):
        timeout = kwargs.pop("timeout", 10.0)
        kwargs["client"] = get_global_client(timeout)
        super().__init__(*args, **kwargs)

# ===================== æ£€æŸ¥è¿œç¨‹ Ollama =====================
async def is_remote_ollama_available(base_url: str, timeout: float = 3.0) -> bool:
    try:
        client = get_global_client(timeout)
        resp = await client.get(f"{base_url}/api/tags")
        return resp.status_code == 200
    except Exception as e:
        logger.info(f"âš ï¸ Remote Ollama not reachable: {e}")
        return False

# ===================== è·å– LLM =====================
async def get_llm() -> DirectChatOllama:
    """
    ä¼˜å…ˆä½¿ç”¨è¿œç¨‹ Ollama æ¨¡å‹ï¼Œå¦‚æœè¿œç¨‹ä¸å¯ç”¨åˆ™å›é€€åˆ°æœ¬åœ°æ¨¡å‹ã€‚
    """
    if await is_remote_ollama_available(REMOTE_OLLAMA_URL):
        logger.info(f"ğŸŒ Using remote Ollama model: {REMOTE_MODEL}")
        return DirectChatOllama(model=REMOTE_MODEL, base_url=REMOTE_OLLAMA_URL)
    else:
        logger.info(f"ğŸ”„ Falling back to local model: {LOCAL_MODEL}")
        return DirectChatOllama(model=LOCAL_MODEL)

# ===================== å®‰å…¨è§£æ JSON =====================
async def safe_llm_parse(prompt: str) -> dict:
    """
    å®‰å…¨è§£æ LLM è¿”å›å†…å®¹ä¸º JSONã€‚
    æ”¯æŒä»¥ä¸‹åœºæ™¯ï¼š
    - æ¨¡å‹è¿”å›çº¯ JSON
    - æ¨¡å‹è¿”å›å‰åå¸¦è§£é‡Šæ–‡å­—
    - æ¨¡å‹è¾“å‡º markdown ä»£ç å—ï¼ˆå¦‚ ```json ... ```ï¼‰
    """
    llm = await get_llm()
    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        response_text = resp.generations[0][0].message.content.strip()
        print(response_text)

        # ğŸ§¹ æ¸…ç†å¸¸è§åŒ…è£¹å­—ç¬¦
        text = response_text.strip()
        text = text.replace("```json", "").replace("```", "").strip()
        text = text.replace("JSON:", "").replace("json:", "").strip()

        # æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}' â€”â€” ä¿è¯å–åˆ°æœ€å¤–å±‚ JSONï¼ˆæ¯”éè´ªå©ªæ­£åˆ™æ›´ç¨³ï¼‰
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(text[start:end+1])
            except json.JSONDecodeError:
                pass

        matches = re.findall(r"\{[\s\S]*?\}", text)
        for m in matches:
            try:
                return json.loads(m)
            except json.JSONDecodeError:
                continue

        # å†å…œåº•ï¼škey:value ç®€å•è§£æï¼ˆä¿å®ˆï¼‰
        pairs = re.findall(r'"(\w+)"\s*:\s*"([^"]*)"', text)
        if pairs:
            return {k: v for k, v in pairs}

        logger.warning("âš ï¸ æœªè¯†åˆ«åˆ° JSON æ ¼å¼ï¼Œè¿”å›ç©º dictã€‚åŸæ–‡: %s", text[:400])
        return {}
    except Exception as e:
        logger.exception("âŒ safe_llm_parse è§£æå¤±è´¥:", e)
        return {}

# ===================== é€šç”¨èŠå¤©å‡½æ•° =====================
async def safe_llm_chat(prompt: str) -> str:
    """
    è®©æ¨¡å‹è‡ªç”±å›ç­”ï¼Œè¿”å›çº¯æ–‡æœ¬ã€‚
    """
    llm = await get_llm()
    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        return resp.generations[0][0].message.content.strip()
    except Exception as e:
        logger.exception("âŒ LLM èŠå¤©å¤±è´¥:", e)
        return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"

# ===================== æ¸…ç†å…¨å±€ AsyncClientï¼ˆç¨‹åºé€€å‡ºæ—¶å¯è°ƒç”¨ï¼‰ =====================
async def close_global_client():
    global _global_client
    if _global_client:
        await _global_client.aclose()
        _global_client = None
