# app/core/llm/llm_client.py
import os
import logging
import re
import json
import asyncio
import httpx
from typing import Optional, Dict, Any
from langchain.schema import HumanMessage

from config import (
    LLM_CHAIN,
    LLM_API_URL,
    LLM_API_KEY,
    LLM_API_TIMEOUT,
    REMOTE_OLLAMA_URL,
    REMOTE_MODEL,
    LOCAL_MODEL,
)

# ===================== å¼ºåˆ¶ç¦ç”¨ç³»ç»Ÿä»£ç† =====================
for key in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]:
    os.environ.pop(key, None)

# ===================== Logger =====================
logger = logging.getLogger("core.llm_client")
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    )

# ===================== ChatOllama å…¼å®¹å¯¼å…¥ =====================
try:
    from langchain_ollama import ChatOllama
    logger.info("âœ… Using ChatOllama from langchain-ollama")
except ImportError:
    try:
        from langchain_community.chat_models import ChatOllama
        logger.info("âœ… Using ChatOllama from langchain_community")
    except ImportError:
        from langchain.chat_models import ChatOllama
        logger.info("âš ï¸ Using ChatOllama from old langchain (may be deprecated)")

# ===================== å…¨å±€ç›´è¿ AsyncClient =====================
_global_client: httpx.AsyncClient | None = None

def get_global_client(timeout: float = 10.0) -> httpx.AsyncClient:
    """
    è¿”å›å…¨å±€å…±äº« AsyncClientï¼Œä¿è¯å®Œå…¨ç›´è¿è¿œç¨‹ Ollamaï¼Œä¸èµ°ç³»ç»Ÿä»£ç†ã€‚
    """
    global _global_client
    if _global_client is None:
        transport = httpx.AsyncHTTPTransport(retries=0)
        _global_client = httpx.AsyncClient(
            timeout=timeout,
            transport=transport,
            trust_env=False,  # â­ ä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†
        )
    return _global_client

# ===================== è‡ªå®šä¹‰ ChatOllama =====================
class DirectChatOllama(ChatOllama):
    """
    å¼ºåˆ¶ç›´è¿è¿œç¨‹ Ollamaï¼Œå®Œå…¨å¿½ç•¥ç³»ç»Ÿä»£ç†ã€‚
    """
    def __init__(self, *args, **kwargs):
        timeout = kwargs.pop("timeout", 10.0)
        kwargs["client"] = get_global_client(timeout)
        super().__init__(*args, **kwargs)

# ===================== æ£€æŸ¥è¿œç¨‹ Ollama =====================
async def is_remote_ollama_available(base_url: str, timeout: float = 3.0) -> bool:
    try:
        client = get_global_client(timeout)
        resp = await client.get(f"{base_url}/api/tags")
        return resp.status_code == 200
    except Exception as e:
        logger.info(f"âš ï¸ Remote Ollama not reachable: {e}")
        return False


# ============================================================
#                STEP 1 â€” API è°ƒç”¨ï¼ˆDify / è‡ªå®šä¹‰ APIï¼‰
# ============================================================
async def _try_api_call(prompt: str) -> Optional[str]:
    if not LLM_API_URL or not LLM_API_KEY:
        return None

    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "inputs": {},
        "query": prompt,
        "response_mode": "blocking",
        "conversation_id": "",
        "user": "py_client"
    }

    timeout = httpx.Timeout(LLM_API_TIMEOUT)

    async with httpx.AsyncClient(timeout=timeout) as client:
        for attempt in range(2):
            try:
                resp = await client.post(LLM_API_URL, headers=headers, json=payload)
                data = resp.json()

                if resp.status_code == 200 and "answer" in data:
                    return data["answer"].strip()

                logger.warning(f"API è¿”å›æ— æ•ˆå†…å®¹: {data}")

            except Exception as e:
                err = type(e).__name__
                msg = str(e).split("\n")[0][:200]
                logger.error(f"API è°ƒç”¨å¤±è´¥: {err} - {msg}")

            await asyncio.sleep(1)

    return None


def _extract_json(text: str) -> Optional[Dict[Any, Any]]:
    if not text:
        return None

    # 1ï¸âƒ£ åˆ é™¤ <think> æ¨ç†å†…å®¹
    text = re.sub(r'<think>.*?</think>\s*', '', text, flags=re.DOTALL).strip()

    # 2ï¸âƒ£ æ¸…ç†å¸¸è§åŒ…è£¹å­—ç¬¦
    text = text.replace("```json", "").replace("```", "").strip()
    text = text.replace("JSON:", "").replace("json:", "").strip()

    # 3ï¸âƒ£ æå–æœ€å¤–å±‚ JSON æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}' â€”â€” ä¿è¯å–åˆ°æœ€å¤–å±‚ JSONï¼ˆæ¯”éè´ªå©ªæ­£åˆ™æ›´ç¨³ï¼‰
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        json_str = text[start:end + 1]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass  # ç»§ç»­èµ°ä¸‹ä¸€æ­¥å…œåº•

    # 4ï¸âƒ£ éè´ªå©ªåŒ¹é…å¤šä¸ª JSONï¼Œå–ç¬¬ä¸€ä¸ªå¯è§£æçš„ï¼ˆæ—§é€»è¾‘ï¼‰
    matches = re.findall(r"\{[\s\S]*?\}", text)
    for m in matches:
        try:
            return json.loads(m)
        except json.JSONDecodeError:
            continue

    # 5ï¸âƒ£ å†å…œåº•ï¼šåŒ¹é… "key": "value" çš„æ ¼å¼ï¼ˆæ—§é€»è¾‘ï¼‰
    pairs = re.findall(r'"(\w+)"\s*:\s*"([^"]*)"', text)
    if pairs:
        return {k: v for k, v in pairs}

    return None



# ============================================================
#    STEP 2 â€” æ„é€ ç»Ÿä¸€ LLMï¼šä¼˜å…ˆ API â†’ remote â†’ local
# ============================================================
async def _get_unified_answer(prompt: str) -> str:
    """
    é€šç”¨ç»Ÿä¸€ LLM è°ƒåº¦ï¼š
    æ ¹æ® LLM_CHAIN = ["api", "remote", "local"]
    æŒ‰é¡ºåºé€çº§å°è¯•ï¼ŒæˆåŠŸåˆ™è¿”å›ã€‚
    """

    for provider in LLM_CHAIN:
        provider = provider.strip()

        # ===========================
        # 1) API è°ƒç”¨
        # ===========================
        if provider == "api":
            if LLM_API_URL and LLM_API_KEY:
                logger.info("ğŸ”Œ å°è¯• API è°ƒç”¨ â€¦")
                ans = await _try_api_call(prompt)
                if ans:
                    logger.info("ğŸŒ API æˆåŠŸ")
                    return ans
                logger.warning("âš ï¸ API å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª provider")
            else:
                logger.warning("âš ï¸ å·²é…ç½® api ä½†ç¼ºå°‘ LLM_API_URL æˆ– LLM_API_KEY")

        # ===========================
        # 2) remote_ollama
        # ===========================
        elif provider == "remote":
            logger.info("ğŸ”Œ æ£€æŸ¥ remote ollama â€¦")
            if await is_remote_ollama_available(REMOTE_OLLAMA_URL):
                try:
                    logger.info(f"ğŸŒ å°è¯• remote ollama: {REMOTE_MODEL}")
                    llm = DirectChatOllama(model=REMOTE_MODEL, base_url=REMOTE_OLLAMA_URL)
                    resp = await llm.agenerate([[HumanMessage(content=prompt)]])
                    return resp.generations[0][0].message.content.strip()
                except Exception as e:
                    logger.warning(f"âš ï¸ remote ollama è°ƒç”¨å¤±è´¥: {e}")
            else:
                logger.warning("âš ï¸ remote ollama ä¸å¯ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª provider")

        # ===========================
        # 3) local_ollama
        # ===========================
        elif provider == "local":
            try:
                logger.info(f"ğŸ’» å°è¯• local ollama: {LOCAL_MODEL}")
                llm = DirectChatOllama(model=LOCAL_MODEL)
                resp = await llm.agenerate([[HumanMessage(content=prompt)]])
                return resp.generations[0][0].message.content.strip()
            except Exception as e:
                logger.warning(f"âš ï¸ local ollama è°ƒç”¨å¤±è´¥: {e}")

        else:
            logger.error(f"âŒ æœªè¯†åˆ«çš„ LLM provider: {provider}")

    # =================================================
    # æ‰€æœ‰ provider å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    # =================================================
    logger.error("âŒ æ‰€æœ‰ provider å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
    return ""



# ============================================================
#                     å¯¹å¤–ç»Ÿä¸€æ¥å£
# ============================================================
async def safe_llm_parse(prompt: str) -> dict:
    """
    ç»Ÿä¸€è§£æä¸º JSONï¼Œå†…éƒ¨è‡ªåŠ¨é€‰æ‹© API / Remote / Local
    """
    answer = await _get_unified_answer(prompt)
    parsed = _extract_json(answer)
    return parsed or {}


async def safe_llm_chat(prompt: str) -> str:
    """
    ç»Ÿä¸€èŠå¤©æ¥å£
    """
    return await _get_unified_answer(prompt)


# ===================== æ¸…ç†å…¨å±€ AsyncClient =====================
async def close_global_client():
    global _global_client
    if _global_client:
        await _global_client.aclose()
        _global_client = None
