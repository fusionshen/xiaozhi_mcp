# rag_web_multi_v2.py
# Streamlit æœ¬åœ° RAGï¼ˆå¤šæ–‡ä»¶ã€å¢é‡å…¥åº“ã€è®°å¿†ã€å¤šè½®ã€éé˜»å¡ï¼‰
import os
import tempfile
import hashlib
import json
import threading
import time
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama

# ========== é…ç½® ==========
EMBEDDING_MODEL = "BAAI/bge-small-zh"
DB_DIR = "./chroma_db"
PROCESSED_META = "./processed_files.json"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
BATCH_SIZE = 64  # æ¯æ¬¡å‘ Chroma æ‰¹é‡å†™å…¥å¤šå°‘ chunkï¼ˆå¯æ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
TOP_K = 3  # æ£€ç´¢ Top-K

# ========== åˆå§‹åŒ– ==========
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
llm = Ollama(model="qwen2.5:1.5b")

# helper: load or init processed file metadata
def load_processed_meta():
    if os.path.exists(PROCESSED_META):
        with open(PROCESSED_META, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_processed_meta(meta):
    with open(PROCESSED_META, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)

# helper: fingerprint a file
def fingerprint(path):
    stat = os.stat(path)
    token = f"{os.path.basename(path)}|{stat.st_size}|{int(stat.st_mtime)}"
    return hashlib.sha256(token.encode("utf-8")).hexdigest()

# load existing vectorstore if present
def load_vectorstore_if_exists():
    if os.path.exists(DB_DIR) and os.listdir(DB_DIR):
        try:
            vs = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)
            return vs
        except Exception as e:
            st.warning(f"åŠ è½½ç°æœ‰å‘é‡åº“å¤±è´¥ï¼ˆå°†é‡å»ºï¼‰ï¼š{e}")
            # fallthrough to None to allow rebuild
    return None

# incremental ingestion: only add chunks from new files
def ingest_files_incremental(file_paths, progress_callback=None):
    """
    file_paths: list of absolute paths to files
    progress_callback: function(progress_fraction, text) for UI updates
    """
    # ensure chroma directory exists
    os.makedirs(DB_DIR, exist_ok=True)

    processed = load_processed_meta()
    new_files = []
    for p in file_paths:
        fid = fingerprint(p)
        if fid not in processed:
            new_files.append((p, fid))
    if not new_files:
        if progress_callback:
            progress_callback(1.0, "æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¯¼å…¥ã€‚")
        return

    # Load or create vectorstore object
    vectorstore = load_vectorstore_if_exists()
    if vectorstore is None:
        # create empty Chroma by writing zero items via from_texts([])
        vectorstore = Chroma.from_texts([], embedding_model, persist_directory=DB_DIR)

    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    total_chunks = 0
    chunks_buffer = []

    # iterate files and chunk them
    for file_idx, (file_path, fid) in enumerate(new_files):
        ext = os.path.splitext(file_path)[1].lower()
        if ext == ".pdf":
            loader = PyPDFLoader(file_path)
        elif ext == ".csv":
            loader = CSVLoader(file_path, encoding="utf-8")
        else:
            loader = TextLoader(file_path, encoding="utf-8")
        docs = loader.load()
        chunks = splitter.split_documents(docs)
        total_chunks += len(chunks)
        # for each chunk add metadata about source file and chunk index
        for i, c in enumerate(chunks):
            meta = {"source": os.path.basename(file_path), "file_fingerprint": fid, "chunk_index": i}
            chunks_buffer.append((c.page_content, meta))

        # flush buffer periodically to control memory/CPU bursts
        while len(chunks_buffer) >= BATCH_SIZE:
            batch = chunks_buffer[:BATCH_SIZE]
            texts = [t for t, m in batch]
            metadatas = [m for t, m in batch]
            # add_texts will compute embeddings then persist
            vectorstore.add_texts(texts=texts, metadatas=metadatas)
            # remove flushed
            chunks_buffer = chunks_buffer[BATCH_SIZE:]
            if progress_callback:
                progress_callback(0.01, f"å·²å†™å…¥éƒ¨åˆ†å—åˆ°å‘é‡åº“ï¼ˆå·²å†™å…¥è‹¥å¹²æ‰¹æ¬¡ï¼‰...")

        # mark file processed
        processed[fid] = {
            "filename": os.path.basename(file_path),
            "size": os.path.getsize(file_path),
            "mtime": int(os.path.getmtime(file_path))
        }
        save_processed_meta(processed)

    # flush remaining
    if chunks_buffer:
        texts = [t for t, m in chunks_buffer]
        metadatas = [m for t, m in chunks_buffer]
        vectorstore.add_texts(texts=texts, metadatas=metadatas)

    # persist Chroma
    if hasattr(vectorstore, "persist"):
        vectorstore.persist()

    if progress_callback:
        progress_callback(1.0, f"å¯¼å…¥å®Œæˆï¼šå…±å¯¼å…¥ {total_chunks} ä¸ªå—ã€‚")

# RAG query: returns (quick_snippet, llm_answer_or_none)
def rag_query_with_quick_snippet(query, vectorstore, history=None, run_llm=False):
    """
    - quick_snippet: top-1 doc snippet (fast)
    - llm_answer: if run_llm True, call LLM with context and return full answer; else None
    """
    # fast similarity search (should be quick if vectorstore loaded)
    docs = vectorstore.similarity_search(query, k=TOP_K)
    if not docs:
        return ("æœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µã€‚", None)

    quick_snippet = docs[0].page_content[:500]  # 500 chars as quick preview

    llm_answer = None
    if run_llm:
        # prepare prompt including chat history (memory)
        history_text = ""
        if history:
            for q, a in history[-6:]:  # æœ€è¿‘6è½®
                history_text += f"ç”¨æˆ·ï¼š{q}\nåŠ©æ‰‹ï¼š{a}\n"
        context = "\n\n".join([f"[æ¥æºï¼š{d.metadata.get('source','unknown')}] {d.page_content}" for d in docs])
        prompt = f"""
ä½ æ˜¯ä¼ä¸šçŸ¥è¯†åŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ç›¸å…³æ–‡æ¡£ç‰‡æ®µä¸å¯¹è¯å†å²ï¼Œè¯·åŸºäºè¿™äº›å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå›ç­”æ—¶ç»™å‡ºå¼•ç”¨æ¥æºï¼ˆæ–‡ä»¶åï¼‰ã€‚
å¯¹è¯å†å²ï¼š
{history_text}

ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºæ–‡æ¡£å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ï¼Œè¯·è¯´æ˜â€œæ–‡æ¡£æœªæ¶‰åŠâ€ã€‚"""
        llm_answer = llm(prompt)
    return quick_snippet, llm_answer

# ========== Streamlit UI ==========
st.set_page_config(page_title="æœ¬åœ° RAGï¼ˆå¢é‡+å¤šæ–‡ä»¶+è®°å¿†ï¼‰", layout="wide")
st.title("ğŸ“˜ æœ¬åœ° RAG çŸ¥è¯†åŠ©æ‰‹ï¼ˆå¢é‡å…¥åº“ + å¤šæ–‡ä»¶ + è®°å¿†ï¼‰")

# show status of existing vector DB
vectorstore = load_vectorstore_if_exists()
if vectorstore:
    st.info("å·²åŠ è½½æœ¬åœ°å‘é‡åº“ï¼ˆå¯ç«‹å³æ£€ç´¢ï¼‰")
else:
    st.info("å½“å‰æ²¡æœ‰æœ¬åœ°å‘é‡åº“ï¼Œä¸Šä¼ æ–‡ä»¶åå°†æ„å»ºç¬¬ä¸€ä¸ªå‘é‡åº“ï¼ˆå¯å¢é‡å¯¼å…¥ï¼‰")

# Session memory for chat history
if "history" not in st.session_state:
    st.session_state["history"] = []  # list of (question, answer)

# file uploader (multiple)
uploaded_files = st.file_uploader("ğŸ“‚ ä¸Šä¼ å¤šä¸ªæ–‡ä»¶ (PDF / TXT / CSV)", accept_multiple_files=True, type=["pdf", "txt", "csv"])

# place to show ingestion progress
progress_text = st.empty()
prog_bar = st.progress(0.0)

# ingestion thread control
if uploaded_files:
    # write uploaded files to temp paths
    tmp_paths = []
    for uf in uploaded_files:
        suffix = os.path.splitext(uf.name)[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(uf.read())
            tmp_paths.append(tmp.name)

    # define progress callback for background ingestion
    def progress_cb(fraction, text):
        prog_bar.progress(min(max(fraction, 0.0), 1.0))
        progress_text.text(text)

    # kick off a background thread to ingest files incrementally
    ingest_thread = threading.Thread(target=ingest_files_incremental, args=(tmp_paths, progress_cb))
    ingest_thread.daemon = True
    ingest_thread.start()
    st.success("å·²å¼€å§‹åå°å¯¼å…¥æ–‡ä»¶ã€‚ç°æœ‰å‘é‡åº“å¯ç»§ç»­è¢«æŸ¥è¯¢ï¼›æ–°æ–‡ä»¶å¯¼å…¥å®Œæˆåä¼šè‡ªåŠ¨å¯ç”¨ã€‚")

# Query area
st.markdown("---")
st.subheader("é—®ç­”åŒºï¼ˆå³æ—¶ç‰‡æ®µ + å¯é€‰å…¨æ–‡ç”Ÿæˆï¼‰")
query = st.text_input("â“ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆå¤šè½®ä¼šè¯ä¼šä¿ç•™å†å²ä¸Šä¸‹æ–‡ï¼‰")

col1, col2 = st.columns([1, 1])
with col1:
    if st.button("å¿«é€Ÿæ£€ç´¢ï¼ˆç§’çº§ï¼Œè¿”å› Top-1 ç‰‡æ®µï¼‰"):
        # ensure vectorstore exists (load again in case background finished)
        vectorstore = load_vectorstore_if_exists()
        if not vectorstore:
            st.error("å½“å‰æ²¡æœ‰å¯ç”¨å‘é‡åº“ï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶ç­‰å¾…å¯¼å…¥å®Œæˆã€‚")
        else:
            quick, _ = rag_query_with_quick_snippet(query, vectorstore, history=st.session_state["history"], run_llm=False)
            st.markdown("**å¿«é€Ÿç‰‡æ®µï¼ˆé¢„è§ˆï¼‰**")
            st.write(quick)

with col2:
    if st.button("ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼ˆå¯èƒ½ >10sï¼‰"):
        vectorstore = load_vectorstore_if_exists()
        if not vectorstore:
            st.error("å½“å‰æ²¡æœ‰å¯ç”¨å‘é‡åº“ï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶ç­‰å¾…å¯¼å…¥å®Œæˆã€‚")
        else:
            with st.spinner("æ­£åœ¨è°ƒç”¨æ¨¡å‹ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼Œå¯èƒ½è€—æ—¶..."):
                quick, full = rag_query_with_quick_snippet(query, vectorstore, history=st.session_state["history"], run_llm=True)
            if full:
                st.markdown("### ğŸ’¡ å®Œæ•´ç­”æ¡ˆ")
                st.write(full)
                # record to history
                st.session_state["history"].append((query, full))
            else:
                st.write("æœªç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼Œå±•ç¤ºå¿«é€Ÿç‰‡æ®µï¼š")
                st.write(quick)

# show chat history (memory)
if st.session_state["history"]:
    st.markdown("---")
    st.subheader("ä¼šè¯å†å²ï¼ˆè®°å¿†ï¼‰")
    for i, (q, a) in enumerate(reversed(st.session_state["history"][-20:])):
        st.markdown(f"**Q:** {q}")
        st.write(f"**A:** {a}")

